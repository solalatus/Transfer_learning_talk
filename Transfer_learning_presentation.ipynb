{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<br>\n",
    "<h1><center> <br> Do we always have to start from scratch?</center></h1>\n",
    "\n",
    "<h2><center>Or is there a better way? </center></h2>\n",
    "\n",
    "\n",
    "\n",
    "<h3><center>Levente Szabados</center></h3>\n",
    "<h3><center>Frankfurt School of Finance and Management</center></h3>\n",
    "<h3><center>AI Partners</center></h3>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Motivation:\n",
    "\n",
    "\"- We would like to have a chatbot!\"\n",
    "\n",
    "\"- Sure! Do you have enough data?\"\n",
    "\n",
    "\"- Absolutely, we are overwhelmed!'\n",
    "    \n",
    "\t(Turns out to be 150 emails...) \n",
    "    \n",
    "\"- OOOK, wel, then, let's see what we can do?\"\n",
    "\n",
    "**Options:**\n",
    "\n",
    "1. Try to learn a whole language with deep meaning from 150 emails?\n",
    "2. Give it up and go for a walk?\n",
    "3. ????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sidenote:\n",
    "\n",
    "It is in itself a more nuanced problem, if we have enough data or not, see:\n",
    "\n",
    "[“Do I have enough data for Machine Learning?” — Um… maybe…?](https://medium.com/@haomiao/do-i-have-enough-data-for-machine-learning-um-maybe-d45f41234d2d)\n",
    "\n",
    "The answer is: **\"It depends.\"**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And it does **not** just depend on dimensionality.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/4470/1*Ox1nUWMV4TUAdb5kOPGYjg.png\" width=55%>\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/4449/1*3BTvgPA-cYQ1y1mTUStcVg.png\" width=55%>\n",
    "\n",
    "You can simply **distinguish peacocks from peahen** by measuring the **amount of green** on the picture - even though the picture is of high dimensionality (as many as number of pixels). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The amount of data is roughly proportionate on **how difficult decision you have to make**.\n",
    "<img src=\"https://miro.medium.com/max/3353/1*pao0zUAyGYuo2uA4tGqWLA.png\" width=45%>\n",
    "<img src=\"https://miro.medium.com/max/3241/1*ENZI_EOozpPDR4-jwrISdw.png\" width=45%>\n",
    "\n",
    "Which in many cases can be only found out by trying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is important to note that **all learning is a form of memorization**.\n",
    "\n",
    "<img src=\"https://datascienceplus.com/wp-content/uploads/2016/12/outliers_effect.png\" width=55%>\n",
    "\n",
    "We would like to \"distill\" the essence, remember what is the **defining characteristic** of the data, and **disregard \"noise\"**, eg. outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This in turn has strong connections with **overfitting**, since if we \"memorize\" also the **\"unimportant\", random properties** of the data, we will have a **bad generalization** performance out of sample\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "<center>\n",
    "<img src=\"https://hackernoon.com/hn-images/1*SBUK2QEfCP-zvJmKm14wGQ.png\" width=55%>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<br> </br>\n",
    "<br> </br>\n",
    "In turn, good generalization is crucial in case of **\"covariate shift\"**, when the data distribution changes **out of sample**.\n",
    "\n",
    "<img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/07230628/plot1-300x200.png\" width=45%>\n",
    "\n",
    "That is: ALWAYS :-)\n",
    "\n",
    "So if we have learned the right function, we can still generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"https://i.ytimg.com/vi/WJjPVj2bBVQ/hqdefault.jpg\" width=50%>\n",
    "</center>\n",
    "    \n",
    "And since Deep Learning models have **huge memory capacity**, we would like to see if they can memorize some **generally useful patterns** across domains!\n",
    "\n",
    "That is: train a model on an abundant, general dataset, (preferredly unsupervised), and apply it to the task at hand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The other answer to the enough data question is: \n",
    "\n",
    "**\"Look at what others did, maybe you get some help!\"**\n",
    "\n",
    "This latter strategy we will follow. :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Static representation transfer:\n",
    "\n",
    "Learn a shallow representation, build a model on it.\n",
    "\n",
    "Example: word2vec + SVM\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/rohan-varma/paper-analysis/master/word2vec-papers/models.png\" width=40%>\n",
    "\n",
    "Mikolov et. al. substituted the large matrix decomposition task in language representation learning to a local context prediction task. \n",
    "\n",
    "The main aim was: **learning a good representation** by solving an (uninteresting) unsupervised problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Representation\n",
    "\n",
    "**Word2vec** as a quasi side-effect of the prediction task learns systematic mapping of word syntax and semantics to a dense vector space.\n",
    "<center>\n",
    "<img src=\"https://miro.medium.com/max/2258/1*2r1yj0zPAuaSGZeQfG6Wtw.png\" width=65%>\n",
    "</center>\n",
    "\n",
    "A stable decision boundary in this space is good possible, eg. with a large margin classifier like SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this approach, representation is passively transferred, it is not modified during the learning of the target task.\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"https://miro.medium.com/max/1121/0*9BJvvSF0PWivkZWT.\" width=55%>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. End-to-end training with transferred representations\n",
    "\n",
    "Example: Initialize embedding layer of a deep network with word2vec, then let it train further\n",
    "<center>\n",
    "<img src=\"https://www.kdnuggets.com/wp-content/uploads/RNN.jpg\" width=45%>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Pro:**\n",
    "\n",
    "- Easy to implement\n",
    "- Gives visible boost to models\n",
    "\n",
    "\n",
    "**Con:**\n",
    "\n",
    "- Only \"shallow\" knowledge has been transferred\n",
    "- With initial updates some of the information gets destroyed\n",
    "\n",
    "**Forgetting starts to pop up!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. Deep representation transfer\n",
    "\n",
    "The general approach on deep representation transfer capitalizes on the fact, that learned representations for neural models are typically having two somewhat distinguishable structural elements:\n",
    "\n",
    "- **the first** (MANY!) **layers are for representation learning** (\"embedding\")\n",
    "- **the last** (typically 1 or so) **layers are for classification** (\"cut\", ie. a linear classifier)\n",
    "\n",
    "With this in mind, we can try to transfer a complete representation \"stack\", and only **replace the last decision layer**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<br></br>\n",
    "<center>\n",
    "<img src=\"https://miro.medium.com/max/565/0*ZwCyanzdxYyWXLKK.png\" width=45%>\n",
    "</center>\n",
    "\n",
    "(Please observe, that nothing prevents us from parallel, multi-task learning at this point!\n",
    " Hence the picture above.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The problem: Catastrophic forgetting\n",
    "<br></br>\n",
    "<center>\n",
    "<img src=\"https://www.researchgate.net/profile/Soheil_Kolouri2/publication/331768722/figure/fig1/AS:736556629381131@1552620193820/Depiction-of-catastrophic-forgetting-in-binary-classification-tasks-when-there-is-a.ppm\" width=55%>\n",
    "</center>\n",
    "\n",
    "Even in the \"natural\" case, things can be easily forgotten.\n",
    "\n",
    "Combine this with the possibility, that the **early gradient updates cause much noise** since the last classification layers can be newly initialized for the task and you will get pretty weak benefit from transfer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# One solution: Gradual unfreezing\n",
    "\n",
    "The idea is pretty simple: for the early part of the transfer learning based training do not allow updates on weights for the majority of the network, just **gradually unfreeze** the layers later on.\n",
    "\n",
    "<img src=\"https://humboldt-wi.github.io/blog/img/seminar/group4_ULMFiT/Figure_21.png\" width=65%>\n",
    "\n",
    "See a detailed analysis of Ruder and co.'s UMLFIT [here](https://medium.com/explorations-in-language-and-learning/transfer-learning-in-nlp-2d09c3dfaeb6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multiple other approaches: Verdict is still out\n",
    "\n",
    "The development of more effective transfer learning methods is far from finished, there are quite [recent papers](https://arxiv.org/abs/1812.01640) which offer a good survey:\n",
    "<center>\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1V03HbhaHSRboTubR5afsMh0G6TTbkMMw\"  width=40%>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Recently a more **systematic analysis of transfer learning with BERT model** has been carried out in context of sentiment classification.\n",
    "\n",
    "Multiple approaches are being studied:\n",
    "<center>\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=12XWIaWTgR16TD6pBQ3Q1uyu1vaZ7vkar\" width=45%>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# It became critical \n",
    "\n",
    "The widespread success of Deep Learning is more and more based on the fact, that:\n",
    "1. _Huge_ pre-trained models are available\n",
    "2. Integration work became more easy, methods standardizing.\n",
    "\n",
    "In fact, a whole \"cottage industry\" has spawn for making the adaptation work easy.\n",
    "\n",
    "Some examples / infrastructure:\n",
    "- [Transfer learning with TensorfLow Hub](https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub)\n",
    "- [BERT Text Classification in 3 Lines of Code Using Keras](https://towardsdatascience.com/bert-text-classification-in-3-lines-of-code-using-keras-264db7e7a358)\n",
    "- [Pre-trained models for speech recognition](https://www.kaggle.com/c/tensorflow-speech-recognition-challenge/discussion/43576)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion:\n",
    "\n",
    "**No. Typically we won't start from scratch!**\n",
    "\n",
    "\"Oh, only 150 emails? No problem! I import pre-trained BERT in 3 lines, and we are good to go!\" :-)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "rise": {
   "footer": "<center>HWSW Mobile! - 2019. november 27-28.</center>",
   "header": "<img src='https://static.wixstatic.com/media/5f2421_869c8c33a7f6462abbc81ea4565c9940~mv2.png/v1/fill/w_236,h_84,al_c,q_80,usm_0.66_1.00_0.01/5f2421_869c8c33a7f6462abbc81ea4565c9940~mv2.webp' style='position: fixed; right:8%; top: 4%' align='right'>",
   "theme": "serif"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
